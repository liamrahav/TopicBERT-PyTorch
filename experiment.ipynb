{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import unittest\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, device='cpu'):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "class TwoWayDict(dict):\n",
    "    # From https://stackoverflow.com/questions/1456373/two-way-reverse-map\n",
    "    def __setitem__(self, key, value):\n",
    "        # Remove any previous connections with these values\n",
    "        if key in self:\n",
    "            del self[key]\n",
    "        if value in self:\n",
    "            del self[value]\n",
    "        dict.__setitem__(self, key, value)\n",
    "        dict.__setitem__(self, value, key)\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        dict.__delitem__(self, self[key])\n",
    "        dict.__delitem__(self, key)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of connections\"\"\"\n",
    "        return dict.__len__(self) // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reuters8 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6241 train sents, 2451 test sents\n",
      "[['&', 'lt', ';', '>', 'needs', 'additional', 'funds', 'electronics', 'corp', 'said', 'it', 'is', 'available', 'means', 'of', 'raising', 'additional', 'funds', 'needed', 'to', 'finance', 'continuing', 'operations', ',', 'but', 'there', 'is', 'no', 'assurance', 'that', 'it', 'will', 'succeed', '.', 'the', 'company', 'said', 'it', 'continues', 'to', 'experience', 'negative', 'cash', 'flow', '.', 'today', 'it', 'reported', 'a', 'loss', 'for', 'the', 'third', 'quarter', 'ended', 'december', '31', 'of', ',', '029', 'dlrs', ',', 'compared', 'with', 'a', '114', ',', '712', 'dlr', 'loss', 'a', 'year', '.', 'said', 'it', 'has', 'received', 'from', 'marine', 'midland', 'banks', 'inc', '&', 'lt', ';', 'mm', '>', 'may', '26', 'on', 'covenants', 'in', 'its', 'loan', 'agreement', '.', 'the', 'company', 'said', 'marine', 'midland', 'has', 'agreed', 'to', 'advance', 'it', 'an', 'additional', 'working', 'capital', 'loan', 'that', 'will', 'be', 'guaranteed', 'by', 'chairman', 'henry', '.', 'loans', 'from', 'marine', 'midland', 'secured', 'by', 'substantially', 'all', 'company', 'assets', '.', 'also', 'said', 'has', 'as', 'president', 'and', 'a', 'director', ',', 'and', 'until', 'a', 'replacement', 'is', 'found', ',', 'will', 'act', 'as', 'chief', 'executive', 'and', 'n', '.', ',', 'formerly', 'chief', 'executive', ',', 'will', 'act', 'as', 'chief', 'operating', 'officer', '.'], tensor([1, 0, 0, 0, 0, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import reuters, stopwords\n",
    "\n",
    "class Reuters8Dataset(Dataset):\n",
    "    TOPICS = tuple('earn,ship,interest,acq,crude,money-fx,grain,trade'.strip().split(','))\n",
    "    # Minimum frequency for word to remain in vocab\n",
    "    F_MIN = 10\n",
    "    \n",
    "    def __init__(self, d_type='train', vocab=None):\n",
    "        try:\n",
    "            nltk.data.find('corpora/reuters.zip')\n",
    "        except LookupError:\n",
    "            print(\"The Reuters corpus needs to be installed from NLTK. \\nNavigate to\", end=' ')\n",
    "            print(\"Corpora --> Reuters in the pop-up window & download it. Then rerun the cell.\")\n",
    "            nltk.download()\n",
    "            raise LookupError(\"Could not find corpora/reuters.zip\")\n",
    "            \n",
    "        if d_type not in ('train', 'test'):\n",
    "            raise ValueError(\"d_type must be one of ['train', 'test']\")\n",
    "            \n",
    "        all_files = reuters.fileids(self.TOPICS)\n",
    "\n",
    "        # Clean multilabel entries from data\n",
    "        to_remove = []\n",
    "        for file in all_files:\n",
    "            cats = reuters.categories(file)\n",
    "            if len(cats) > 1 and all([x in self.TOPICS for x in cats]):\n",
    "                to_remove.append(file)\n",
    "\n",
    "        self.files = [file for file in all_files if d_type in file and file not in to_remove]\n",
    "        \n",
    "        if vocab != None:\n",
    "            if not isinstance(vocab, list):\n",
    "                raise TypeError(\"Vocab should be a list of strings that constrains sentences\")\n",
    "            vocab = vocab\n",
    "        else:\n",
    "            self._build_vocab()\n",
    "                        \n",
    "    def _build_vocab(self):\n",
    "        # Remove all words with frequency < F_MIN\n",
    "        stops = stopwords.words('english')\n",
    "        # Lowercase all words, remove stop words\n",
    "        words_clean = [word.lower() for file in self.files for word in reuters.words(file) if word not in stops]\n",
    "        counts = Counter(words_clean)\n",
    "        self.vocab = []\n",
    "        for word in counts:\n",
    "            if counts[word] >= self.F_MIN:\n",
    "                self.vocab.append(word)\n",
    "\n",
    "    @property\n",
    "    def label_mapping(self):\n",
    "        # establish label map for one-hot encoding\n",
    "        label_mapping = TwoWayDict()\n",
    "        for i, label in enumerate(self.TOPICS):\n",
    "            label_mapping[label] = i\n",
    "            \n",
    "        return label_mapping\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sent = [word for word in [x.lower() for x in reuters.words(self.files[index])] if word in self.vocab]\n",
    "        lbl = reuters.categories(self.files[index])[0]\n",
    "        lbl_one_hot = torch.full((len(self.TOPICS),), 0)\n",
    "        lbl_one_hot[self.label_mapping[lbl]] = 1\n",
    "        \n",
    "        return [sent, lbl_one_hot]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "        \n",
    "r8_train = Reuters8Dataset()\n",
    "r8_test = Reuters8Dataset(d_type='test', vocab=r8_train.vocab)\n",
    "\n",
    "print('{} train sents, {} test sents'.format(len(r8_train), len(r8_test)))\n",
    "\n",
    "print(r8_train[49])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is SST-2 Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SST-2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67349 train sents, 872 val sents\n"
     ]
    }
   ],
   "source": [
    "def read_sst2_file(path):\n",
    "  examples = []\n",
    "  with open(path) as f:\n",
    "    f.readline() \n",
    "    for line in f:\n",
    "      sent, label = line.split('\\t')\n",
    "      examples.append([sent, int(label)])\n",
    "  return examples\n",
    "\n",
    "class SST2Dataset(Dataset):\n",
    "  def __init__(self, filename):\n",
    "    path = os.path.join(os.getcwd() + '/datasets/SST-2', filename)\n",
    "    self.examples = read_sst2_file(path)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.examples)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.examples[index]\n",
    "\n",
    "dataset_train = SST2Dataset('train.tsv')\n",
    "dataset_val = SST2Dataset('dev.tsv')\n",
    "print('{} train sents, {} val sents'.format(len(dataset_train), len(dataset_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pretrained tokenizer in bert-base-uncased has vocab size 30522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print('The pretrained tokenizer in bert-base-uncased has vocab size {:d}\\n'.format(tokenizer.vocab_size))\n",
    "\n",
    "# We need to provide a custom collate function to DataLoader because we're handling sentences of varying lengths. \n",
    "def collate_fn(batch):\n",
    "  # https://huggingface.co/transformers/preprocessing.html\n",
    "  sents, labels = zip(*batch)\n",
    "  labels = torch.FloatTensor(labels)\n",
    "  encoded = tokenizer(sents, padding=True, return_tensors='pt')\n",
    "  return encoded['input_ids'], encoded['attention_mask'], labels\n",
    "\n",
    "set_seed(42)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=2, shuffle=False, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_transformer(transformer):\n",
    "  \"\"\"\n",
    "  Initialization scheme used for transformers:\n",
    "  https://huggingface.co/transformers/_modules/transformers/modeling_bert.html\n",
    "  \"\"\"\n",
    "  def init_transformer(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "        module.weight.data.normal_(mean=0.0, std=transformer.config.initializer_range)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        module.bias.data.zero_()\n",
    "        module.weight.data.fill_(1.0)\n",
    "    if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "        module.bias.data.zero_()\n",
    "\n",
    "  return init_transformer\n",
    "\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, drop=0.1):\n",
    "    super().__init__()\n",
    "    self.encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "    self.score = nn.Sequential(nn.Dropout(drop), \n",
    "                               nn.Linear(self.encoder.config.hidden_size, 1))\n",
    "    self.score.apply(get_init_transformer(self.encoder))  # Important to initialize any additional weights the same way as pretrained encoder. \n",
    "    self.loss = nn.BCEWithLogitsLoss(reduction='sum') \n",
    "\n",
    "  def forward(self, input_ids, attention_mask, labels):\n",
    "    hiddens_last = self.encoder(input_ids, attention_mask=attention_mask)[0]  # (batch_size, length, dim), these are last layer embeddings\n",
    "    embs = hiddens_last[:,0,:]  # [CLS] token embeddings\n",
    "    logits = self.score(embs).squeeze(1)  # batch_size\n",
    "    loss_total = self.loss(logits, labels)\n",
    "    return logits, loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 109483009 parameters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_params(model):\n",
    "  return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "model = BertClassifier()\n",
    "print('Model has {} parameters\\n'.format(count_params(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimization(model, num_train_steps, num_warmup_steps, lr, weight_decay=0.01):  \n",
    "  # Copied from: https://huggingface.co/transformers/training.html\n",
    "  no_decay = ['bias', 'LayerNorm.weight']\n",
    "  optimizer_grouped_parameters = [{'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \n",
    "                                   'weight_decay': weight_decay},\n",
    "                                  {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                                   'weight_decay': 0.}]\n",
    "  optimizer = AdamW(optimizer_grouped_parameters, lr=lr)  \n",
    "  scheduler = get_linear_schedule_with_warmup(optimizer, num_training_steps=num_train_steps, num_warmup_steps=num_warmup_steps) \n",
    "  return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_val(model, device):\n",
    "  num_correct_val = 0\n",
    "  model.eval()  \n",
    "  with torch.no_grad(): \n",
    "    for input_ids, attention_mask, labels in dataloader_val:\n",
    "      input_ids = input_ids.to(device)\n",
    "      attention_mask = attention_mask.to(device)\n",
    "      labels = labels.to(device)\n",
    "      logits, _ = model(input_ids, attention_mask, labels) \n",
    "      preds = torch.where(logits > 0., 1, 0)  # 1 if p(1|x) > 0.5, 0 else\n",
    "      num_correct_val += (preds == labels).sum()\n",
    "  acc_val = num_correct_val / len(dataloader_val.dataset) * 100.\n",
    "  return acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, batch_size=32, num_warmup_steps=10, lr=0.00005, num_epochs=3, clip=1., verbose=True, device='cpu'):\n",
    "  model = model.to(device)  # Move the model to device.  \n",
    "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=collate_fn) \n",
    "  num_train_steps = len(dataset_train) // batch_size * num_epochs\n",
    "  optimizer, scheduler = configure_optimization(model, num_train_steps, num_warmup_steps, lr)\n",
    "\n",
    "  loss_avg = float('inf')\n",
    "  acc_train = 0.\n",
    "  best_acc_val = 0.\n",
    "  for epoch in range(num_epochs):\n",
    "    model.train()  # This turns on the training mode (e.g., enable dropout).\n",
    "    loss_total = 0.\n",
    "    num_correct_train = 0\n",
    "    for batch_ind, (input_ids, attention_mask, labels) in enumerate(dataloader_train):\n",
    "      if (batch_ind + 1) % 200 == 0: \n",
    "        print(batch_ind + 1, '/', len(dataloader_train), 'batches done')\n",
    "      input_ids = input_ids.to(device).long()\n",
    "      attention_mask = attention_mask.to(device)\n",
    "      labels = labels.to(device)      \n",
    "      logits, loss_batch_total = model(input_ids, attention_mask, labels) \n",
    "      preds = torch.where(logits > 0., 1, 0)  # 1 if p(1|x) > 0.5, 0 else\n",
    "      num_correct_train += (preds == labels).sum()\n",
    "      loss_total += loss_batch_total.item()            \n",
    "      \n",
    "      loss_batch_avg = loss_batch_total / input_ids.size(0)  \n",
    "      loss_batch_avg.backward()  \n",
    "\n",
    "      if clip > 0.:  # Optional gradient clipping\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "      optimizer.step()  # optimizer updates model weights based on stored gradients\n",
    "      scheduler.step()  # Update lr. \n",
    "      optimizer.zero_grad()  # Reset gradient slots to zero\n",
    "\n",
    "    # Useful training information\n",
    "    loss_avg = loss_total / len(dataloader_train.dataset)\n",
    "    acc_train = num_correct_train / len(dataloader_train.dataset) * 100.\n",
    "\n",
    "    # Check validation performance at the end of every epoch. \n",
    "    acc_val = get_acc_val(model, device)\n",
    "\n",
    "    if verbose:\n",
    "      print('Epoch {:3d} | avg loss {:8.4f} | train acc {:2.2f} | val acc {:2.2f}'.format(epoch + 1, loss_avg, acc_train, acc_val))\n",
    "\n",
    "    if acc_val > best_acc_val: \n",
    "      best_acc_val = acc_val\n",
    "  \n",
    "  if verbose: \n",
    "    print('Final avg loss {:8.4f} | final train acc {:2.2f} | best val acc {:2.2f}'.format(loss_avg, acc_train, best_acc_val))\n",
    "\n",
    "  return best_acc_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below Runs BERT on SST-2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 / 2105 batches done\n",
      "400 / 2105 batches done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-d3b3114f99c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mbest_acc_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-1925e002552f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, batch_size, num_warmup_steps, lr, num_epochs, clip, verbose, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mloss_batch_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch_total\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m       \u001b[0mloss_batch_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mclip\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Optional gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/topicbert/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/topicbert/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if True: # Set True to run. \n",
    "  set_seed(42)\n",
    "  model = BertClassifier()\n",
    "  best_acc_val = train(model, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
