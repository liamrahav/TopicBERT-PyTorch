{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import unittest\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, device='cpu'):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "class TwoWayDict(dict):\n",
    "    # From https://stackoverflow.com/questions/1456373/two-way-reverse-map\n",
    "    def __setitem__(self, key, value):\n",
    "        # Remove any previous connections with these values\n",
    "        if key in self:\n",
    "            del self[key]\n",
    "        if value in self:\n",
    "            del self[value]\n",
    "        dict.__setitem__(self, key, value)\n",
    "        dict.__setitem__(self, value, key)\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        dict.__delitem__(self, self[key])\n",
    "        dict.__delitem__(self, key)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of connections\"\"\"\n",
    "        return dict.__len__(self) // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reuters8 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sent/label pair — Ex #99:\n",
      "\t- sent: stone inc sets quarterly qtly div cts vs cts prior pay may record april one reuter\n",
      "\t- one-hot: tensor([0, 0, 1, 0, 0, 0, 0, 0])\n",
      "\t- int: 2\n",
      "\t- str: earn\n"
     ]
    }
   ],
   "source": [
    "class Reuters8Dataset(Dataset):\n",
    "    '''Initializes a PyTorch Dataset with the appropriate Reuters8 data.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): The name of the file containing Ruters8 label/sentence pairs as TSV. This should be \n",
    "            one of 'training', 'test', 'validation'. The files must be located at <cwd>/datasets/Reuters8/\n",
    "        label_filepath (:obj:`str`, optional): The label file containing the label ordering, separated by \n",
    "            newlines. By default, this looks to <cwd>/datasets/Reuters8/labels.txt. \n",
    "        one_hot (:obj:`bool`, optional): Set to True by default. Determines if labels are one-hot encoded \n",
    "            vectors (True) or topic strings (False)\n",
    "        f_min (:obj:`int`, optional): Set to 7 by default. We remove all words with frequency < f_min.\n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def _read_reuters_tsv(path):\n",
    "        pairs = []\n",
    "        with open(path) as f:\n",
    "            f.readline()\n",
    "            for line in f:\n",
    "                label, sent = line.split('\\t')\n",
    "                pairs.append([sent, label])\n",
    "                \n",
    "        return pairs\n",
    "    \n",
    "    def _clean_examples(self, examples):\n",
    "        # lowercases, removes NLTK stopwords, and removes words appearing <f_min times            \n",
    "        cleaned = []\n",
    "        stops = stopwords.words('english')\n",
    "        for ex in examples:\n",
    "            sent, label = ex\n",
    "            new_sent = ' '.join([x.lower() for x in sent.split(' ') if x.lower() not in stops])\n",
    "            cleaned.append([new_sent, label])\n",
    "            \n",
    "        counts = Counter([split for x in [y[0] for y in cleaned] for split in x.split(' ')])\n",
    "        final = []\n",
    "        for ex in cleaned:\n",
    "            sent, label = ex\n",
    "            new_sent = ' '.join([x for x in sent.split(' ') if counts[x] >= self.f_min])\n",
    "            final.append([new_sent, label])\n",
    "            \n",
    "        return final\n",
    "        \n",
    "    \n",
    "    def __init__(self, filename, label_filepath=None, one_hot=True, f_min=7):\n",
    "        self.f_min = f_min\n",
    "        if label_filepath is None:\n",
    "            label_filepath = os.path.join(os.getcwd(), 'datasets/Reuters8/', 'labels.txt')\n",
    "            \n",
    "        with open(label_filepath) as f:\n",
    "            self.labels = [x.strip() for x in f.read().strip().split('\\n')]\n",
    "    \n",
    "        self.label_mapping = TwoWayDict()\n",
    "        for i, label in enumerate(self.labels):\n",
    "            self.label_mapping[label] = i\n",
    "            \n",
    "        tsv_path = os.path.join(os.getcwd(), 'datasets/Reuters8/', filename)\n",
    "        self.examples = self._clean_examples(Reuters8Dataset._read_reuters_tsv(tsv_path))\n",
    "        \n",
    "        if one_hot:\n",
    "            for i, ex in enumerate(self.examples):\n",
    "                _, label = ex\n",
    "                lbl_one_hot = torch.full((len(self.labels),), 0)\n",
    "                lbl_one_hot[self.label_mapping[label]] = 1\n",
    "                self.examples[i][1] =  lbl_one_hot\n",
    "            \n",
    "            \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.examples[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "                \n",
    "        \n",
    "r8_train = Reuters8Dataset('training.tsv')\n",
    "r8_val = Reuters8Dataset('validation.tsv')\n",
    "r8_test = Reuters8Dataset('test.tsv')\n",
    "\n",
    "if True: # Set to false to not show example\n",
    "    ex = 99\n",
    "    print('Sample sent/label pair — Ex #{}:'.format(ex))\n",
    "    print('\\t- sent:',r8_train[ex][0].strip()) \n",
    "    print('\\t- one-hot:',r8_train[ex][1]) \n",
    "    # convert one-hot label to int & string\n",
    "    lbl_idx = torch.where(r8_train[ex][1] > 0)[0].item()\n",
    "    print('\\t- int:',lbl_idx) \n",
    "    print('\\t- str:',r8_train.label_mapping[lbl_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  3001, 10710,  3240,  2275, 14790,  3001,  4297,  2056,  3530,\n",
      "         4965,  7473,  2102,  3037,  2194,  5271, 29094, 10710,  3240,  2056,\n",
      "         2109, 10710,  3240, 29094,  2089,  2393,  2225,  2446,  2056, 29094,\n",
      "         2081, 10710,  3240,  2194,  2036,  2056,  2171, 13058,  3001,  2056,\n",
      "         7473,  2102,  8406,  1057,  2241,  2522, 10710,  3240,  8727,  3588,\n",
      "         6661,  2194,  2275, 29094,  3688,  2081, 10710,  3240,  3820,  2048,\n",
      "        19875,  2078, 21469,  2099,  7909, 10710,  3240,  2034, 21469,  2099,\n",
      "         7909,  2349,  2420,  3688,  4844,  2833,  3447,  2056,  3488,  3006,\n",
      "        24273,  6226,  2028,  2194,  2056,  2747,  2109,  1057, 29094,  2521,\n",
      "        10710,  3240, 29094,  2853,  1057,  2171,  4353,  3206,  3688,  2093,\n",
      "         2086,  2028,  2095,  6993,  2194,  2056,  2056, 29094,  2747,  2853,\n",
      "         5096,  2647,  3032,  2056,  2747,  4219,  2191, 10504,  4353,  3820,\n",
      "         4636,  5821,  6226,  2194,  2036,  2056,  7566,  9878,  2028,  2218,\n",
      "         3316,  2210,  7045,  7654,  3001,  3037,  2052, 29454, 12926,  2783,\n",
      "         7473,  2102,  2128, 19901,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0])\n",
      "['[CLS]', 'systems', 'ink', '##ey', 'set', 'pact', 'systems', 'inc', 'said', 'agreed', 'buy', 'pc', '##t', 'interest', 'company', 'sell', 'condoms', 'ink', '##ey', 'said', 'used', 'ink', '##ey', 'condoms', 'may', 'help', 'west', 'german', 'said', 'condoms', 'made', 'ink', '##ey', 'company', 'also', 'said', 'name', 'corp', 'systems', 'said', 'pc', '##t', 'stake', 'u', 'based', 'co', 'ink', '##ey', 'affiliate', 'remaining', 'shares', 'company', 'set', 'condoms', 'products', 'made', 'ink', '##ey', 'agreement', 'two', 'ml', '##n', 'dl', '##r', 'payment', 'ink', '##ey', 'first', 'dl', '##r', 'payment', 'due', 'days', 'products', 'approved', 'food', 'administration', 'said', 'plans', 'market', 'expects', 'approval', 'one', 'company', 'said', 'currently', 'used', 'u', 'condoms', 'far', 'ink', '##ey', 'condoms', 'sold', 'u', 'name', 'distribution', 'contract', 'products', 'three', 'years', 'one', 'year', 'periods', 'company', 'said', 'said', 'condoms', 'currently', 'sold', 'sale', 'european', 'countries', 'said', 'currently', 'resources', 'make', 'payments', 'distribution', 'agreement', 'fund', 'marketing', 'approval', 'company', 'also', 'said', 'talks', 'acquire', 'one', 'held', 'companies', 'little', 'assets', 'acquisition', 'systems', 'interest', 'would', 'dil', '##uted', 'current', 'pc', '##t', 're', '##uter', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[CLS] systems inkey set pact systems inc said agreed buy pct interest company sell condoms inkey said used inkey condoms may help west german said condoms made inkey company also said name corp systems said pct stake u based co inkey affiliate remaining shares company set condoms products made inkey agreement two mln dlr payment inkey first dlr payment due days products approved food administration said plans market expects approval one company said currently used u condoms far inkey condoms sold u name distribution contract products three years one year periods company said said condoms currently sold sale european countries said currently resources make payments distribution agreement fund marketing approval company also said talks acquire one held companies little assets acquisition systems interest would diluted current pct reuter [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# We need to provide a custom collate function to DataLoader because we're handling sentences of varying lengths. \n",
    "def collate_fn(batch):\n",
    "  # https://huggingface.co/transformers/preprocessing.html\n",
    "  sents, labels = zip(*batch)\n",
    "  labels = tuple([x.long() for x in labels])\n",
    "  encoded = tokenizer(sents, padding=True, return_tensors='pt')\n",
    "  return encoded['input_ids'], encoded['attention_mask'], labels\n",
    "\n",
    "set_seed(42)\n",
    "dataloader_val = DataLoader(r8_val, batch_size=4, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "if True: # Set to false to not show example\n",
    "    for i, (input_ids, attention_mask, labels) in enumerate(dataloader_val):\n",
    "      if i == 2:\n",
    "        print(input_ids[3])\n",
    "        print(tokenizer.convert_ids_to_tokens(input_ids[3]))\n",
    "        print(tokenizer.decode(input_ids[3]))\n",
    "        print(attention_mask[3])\n",
    "        print(labels[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is SST-2 Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SST-2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['will find little of interest in this film , which is often preachy and poorly acted ', 0]\n",
      "67349 train sents, 872 val sents\n"
     ]
    }
   ],
   "source": [
    "def read_sst2_file(path):\n",
    "  examples = []\n",
    "  with open(path) as f:\n",
    "    f.readline() \n",
    "    for line in f:\n",
    "      sent, label = line.split('\\t')\n",
    "      examples.append([sent, int(label)])\n",
    "  return examples\n",
    "\n",
    "class SST2Dataset(Dataset):\n",
    "  def __init__(self, filename):\n",
    "    path = os.path.join(os.getcwd() + '/datasets/SST-2', filename)\n",
    "    self.examples = read_sst2_file(path)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.examples)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.examples[index]\n",
    "\n",
    "dataset_train = SST2Dataset('train.tsv')\n",
    "dataset_val = SST2Dataset('dev.tsv')\n",
    "print(dataset_train[32])\n",
    "print('{} train sents, {} val sents'.format(len(dataset_train), len(dataset_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pretrained tokenizer in bert-base-uncased has vocab size 30522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print('The pretrained tokenizer in bert-base-uncased has vocab size {:d}\\n'.format(tokenizer.vocab_size))\n",
    "\n",
    "# We need to provide a custom collate function to DataLoader because we're handling sentences of varying lengths. \n",
    "def collate_fn(batch):\n",
    "  # https://huggingface.co/transformers/preprocessing.html\n",
    "  sents, labels = zip(*batch)\n",
    "  labels = torch.FloatTensor(labels)\n",
    "  encoded = tokenizer(sents, padding=True, return_tensors='pt')\n",
    "  return encoded['input_ids'], encoded['attention_mask'], labels\n",
    "\n",
    "set_seed(42)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=2, shuffle=False, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_transformer(transformer):\n",
    "  \"\"\"\n",
    "  Initialization scheme used for transformers:\n",
    "  https://huggingface.co/transformers/_modules/transformers/modeling_bert.html\n",
    "  \"\"\"\n",
    "  def init_transformer(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "        module.weight.data.normal_(mean=0.0, std=transformer.config.initializer_range)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        module.bias.data.zero_()\n",
    "        module.weight.data.fill_(1.0)\n",
    "    if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "        module.bias.data.zero_()\n",
    "\n",
    "  return init_transformer\n",
    "\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, drop=0.1):\n",
    "    super().__init__()\n",
    "    self.encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "    self.score = nn.Sequential(nn.Dropout(drop), \n",
    "                               nn.Linear(self.encoder.config.hidden_size, 1))\n",
    "    self.score.apply(get_init_transformer(self.encoder))  # Important to initialize any additional weights the same way as pretrained encoder. \n",
    "    self.loss = nn.BCEWithLogitsLoss(reduction='sum') \n",
    "\n",
    "  def forward(self, input_ids, attention_mask, labels):\n",
    "    hiddens_last = self.encoder(input_ids, attention_mask=attention_mask)[0]  # (batch_size, length, dim), these are last layer embeddings\n",
    "    embs = hiddens_last[:,0,:]  # [CLS] token embeddings\n",
    "    logits = self.score(embs).squeeze(1)  # batch_size\n",
    "    loss_total = self.loss(logits, labels)\n",
    "    return logits, loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 109483009 parameters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_params(model):\n",
    "  return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "model = BertClassifier()\n",
    "print('Model has {} parameters\\n'.format(count_params(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimization(model, num_train_steps, num_warmup_steps, lr, weight_decay=0.01):  \n",
    "  # Copied from: https://huggingface.co/transformers/training.html\n",
    "  no_decay = ['bias', 'LayerNorm.weight']\n",
    "  optimizer_grouped_parameters = [{'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \n",
    "                                   'weight_decay': weight_decay},\n",
    "                                  {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                                   'weight_decay': 0.}]\n",
    "  optimizer = AdamW(optimizer_grouped_parameters, lr=lr)  \n",
    "  scheduler = get_linear_schedule_with_warmup(optimizer, num_training_steps=num_train_steps, num_warmup_steps=num_warmup_steps) \n",
    "  return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_val(model, device):\n",
    "  num_correct_val = 0\n",
    "  model.eval()  \n",
    "  with torch.no_grad(): \n",
    "    for input_ids, attention_mask, labels in dataloader_val:\n",
    "      input_ids = input_ids.to(device)\n",
    "      attention_mask = attention_mask.to(device)\n",
    "      labels = labels.to(device)\n",
    "      logits, _ = model(input_ids, attention_mask, labels) \n",
    "      preds = torch.where(logits > 0., 1, 0)  # 1 if p(1|x) > 0.5, 0 else\n",
    "      num_correct_val += (preds == labels).sum()\n",
    "  acc_val = num_correct_val / len(dataloader_val.dataset) * 100.\n",
    "  return acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, batch_size=32, num_warmup_steps=10, lr=0.00005, num_epochs=3, clip=1., verbose=True, device='cpu'):\n",
    "  model = model.to(device)  # Move the model to device.  \n",
    "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=collate_fn) \n",
    "  num_train_steps = len(dataset_train) // batch_size * num_epochs\n",
    "  optimizer, scheduler = configure_optimization(model, num_train_steps, num_warmup_steps, lr)\n",
    "\n",
    "  loss_avg = float('inf')\n",
    "  acc_train = 0.\n",
    "  best_acc_val = 0.\n",
    "  for epoch in range(num_epochs):\n",
    "    model.train()  # This turns on the training mode (e.g., enable dropout).\n",
    "    loss_total = 0.\n",
    "    num_correct_train = 0\n",
    "    for batch_ind, (input_ids, attention_mask, labels) in enumerate(dataloader_train):\n",
    "      if (batch_ind + 1) % 200 == 0: \n",
    "        print(batch_ind + 1, '/', len(dataloader_train), 'batches done')\n",
    "      input_ids = input_ids.to(device).long()\n",
    "      attention_mask = attention_mask.to(device)\n",
    "      labels = labels.to(device)      \n",
    "      logits, loss_batch_total = model(input_ids, attention_mask, labels) \n",
    "      preds = torch.where(logits > 0., 1, 0)  # 1 if p(1|x) > 0.5, 0 else\n",
    "      num_correct_train += (preds == labels).sum()\n",
    "      loss_total += loss_batch_total.item()            \n",
    "      \n",
    "      loss_batch_avg = loss_batch_total / input_ids.size(0)  \n",
    "      loss_batch_avg.backward()  \n",
    "\n",
    "      if clip > 0.:  # Optional gradient clipping\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "      optimizer.step()  # optimizer updates model weights based on stored gradients\n",
    "      scheduler.step()  # Update lr. \n",
    "      optimizer.zero_grad()  # Reset gradient slots to zero\n",
    "\n",
    "    # Useful training information\n",
    "    loss_avg = loss_total / len(dataloader_train.dataset)\n",
    "    acc_train = num_correct_train / len(dataloader_train.dataset) * 100.\n",
    "\n",
    "    # Check validation performance at the end of every epoch. \n",
    "    acc_val = get_acc_val(model, device)\n",
    "\n",
    "    if verbose:\n",
    "      print('Epoch {:3d} | avg loss {:8.4f} | train acc {:2.2f} | val acc {:2.2f}'.format(epoch + 1, loss_avg, acc_train, acc_val))\n",
    "\n",
    "    if acc_val > best_acc_val: \n",
    "      best_acc_val = acc_val\n",
    "  \n",
    "  if verbose: \n",
    "    print('Final avg loss {:8.4f} | final train acc {:2.2f} | best val acc {:2.2f}'.format(loss_avg, acc_train, best_acc_val))\n",
    "\n",
    "  return best_acc_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below Runs BERT on SST-2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 / 2105 batches done\n",
      "400 / 2105 batches done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-d3b3114f99c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mbest_acc_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-1925e002552f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, batch_size, num_warmup_steps, lr, num_epochs, clip, verbose, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mloss_batch_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch_total\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m       \u001b[0mloss_batch_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mclip\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Optional gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/topicbert/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/topicbert/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if True: # Set True to run. \n",
    "  set_seed(42)\n",
    "  model = BertClassifier()\n",
    "  best_acc_val = train(model, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
