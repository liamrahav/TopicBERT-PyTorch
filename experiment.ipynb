{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import unittest\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def set_seed(seed, device='cpu'):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67349 train sents, 872 val sents\n"
     ]
    }
   ],
   "source": [
    "def read_sst2_file(path):\n",
    "  examples = []\n",
    "  with open(path) as f:\n",
    "    f.readline() \n",
    "    for line in f:\n",
    "      sent, label = line.split('\\t')\n",
    "      examples.append([sent, int(label)])\n",
    "  return examples\n",
    "\n",
    "class SST2Dataset(Dataset):\n",
    "  def __init__(self, filename):\n",
    "    path = os.path.join(os.getcwd() + '/datasets/SST-2', filename)\n",
    "    self.examples = read_sst2_file(path)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.examples)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.examples[index]\n",
    "\n",
    "dataset_train = SST2Dataset('train.tsv')\n",
    "dataset_val = SST2Dataset('dev.tsv')\n",
    "print('{} train sents, {} val sents'.format(len(dataset_train), len(dataset_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pretrained tokenizer in bert-base-uncased has vocab size 30522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print('The pretrained tokenizer in bert-base-uncased has vocab size {:d}\\n'.format(tokenizer.vocab_size))\n",
    "\n",
    "# We need to provide a custom collate function to DataLoader because we're handling sentences of varying lengths. \n",
    "def collate_fn(batch):\n",
    "  # https://huggingface.co/transformers/preprocessing.html\n",
    "  sents, labels = zip(*batch)\n",
    "  labels = torch.FloatTensor(labels)\n",
    "  encoded = tokenizer(sents, padding=True, return_tensors='pt')\n",
    "  return encoded['input_ids'], encoded['attention_mask'], labels\n",
    "\n",
    "set_seed(42)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=2, shuffle=False, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_transformer(transformer):\n",
    "  \"\"\"\n",
    "  Initialization scheme used for transformers:\n",
    "  https://huggingface.co/transformers/_modules/transformers/modeling_bert.html\n",
    "  \"\"\"\n",
    "  def init_transformer(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "        module.weight.data.normal_(mean=0.0, std=transformer.config.initializer_range)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        module.bias.data.zero_()\n",
    "        module.weight.data.fill_(1.0)\n",
    "    if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "        module.bias.data.zero_()\n",
    "\n",
    "  return init_transformer\n",
    "\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, drop=0.1):\n",
    "    super().__init__()\n",
    "    self.encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "    self.score = nn.Sequential(nn.Dropout(drop), \n",
    "                               nn.Linear(self.encoder.config.hidden_size, 1))\n",
    "    self.score.apply(get_init_transformer(self.encoder))  # Important to initialize any additional weights the same way as pretrained encoder. \n",
    "    self.loss = nn.BCEWithLogitsLoss(reduction='sum') \n",
    "\n",
    "  def forward(self, input_ids, attention_mask, labels):\n",
    "    hiddens_last = self.encoder(input_ids, attention_mask=attention_mask)[0]  # (batch_size, length, dim), these are last layer embeddings\n",
    "    embs = hiddens_last[:,0,:]  # [CLS] token embeddings\n",
    "    logits = self.score(embs).squeeze(1)  # batch_size\n",
    "    loss_total = self.loss(logits, labels)\n",
    "    return logits, loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 109483009 parameters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_params(model):\n",
    "  return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "model = BertClassifier()\n",
    "print('Model has {} parameters\\n'.format(count_params(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimization(model, num_train_steps, num_warmup_steps, lr, weight_decay=0.01):  \n",
    "  # Copied from: https://huggingface.co/transformers/training.html\n",
    "  no_decay = ['bias', 'LayerNorm.weight']\n",
    "  optimizer_grouped_parameters = [{'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \n",
    "                                   'weight_decay': weight_decay},\n",
    "                                  {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                                   'weight_decay': 0.}]\n",
    "  optimizer = AdamW(optimizer_grouped_parameters, lr=lr)  \n",
    "  scheduler = get_linear_schedule_with_warmup(optimizer, num_training_steps=num_train_steps, num_warmup_steps=num_warmup_steps) \n",
    "  return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_val(model, device):\n",
    "  num_correct_val = 0\n",
    "  model.eval()  \n",
    "  with torch.no_grad(): \n",
    "    for input_ids, attention_mask, labels in dataloader_val:\n",
    "      input_ids = input_ids.to(device)\n",
    "      attention_mask = attention_mask.to(device)\n",
    "      labels = labels.to(device)\n",
    "      logits, _ = model(input_ids, attention_mask, labels) \n",
    "      preds = torch.where(logits > 0., 1, 0)  # 1 if p(1|x) > 0.5, 0 else\n",
    "      num_correct_val += (preds == labels).sum()\n",
    "  acc_val = num_correct_val / len(dataloader_val.dataset) * 100.\n",
    "  return acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, batch_size=32, num_warmup_steps=10, lr=0.00005, num_epochs=3, clip=1., verbose=True, device='cpu'):\n",
    "  model = model.to(device)  # Move the model to device.  \n",
    "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=collate_fn) \n",
    "  num_train_steps = len(dataset_train) // batch_size * num_epochs\n",
    "  optimizer, scheduler = configure_optimization(model, num_train_steps, num_warmup_steps, lr)\n",
    "\n",
    "  loss_avg = float('inf')\n",
    "  acc_train = 0.\n",
    "  best_acc_val = 0.\n",
    "  for epoch in range(num_epochs):\n",
    "    model.train()  # This turns on the training mode (e.g., enable dropout).\n",
    "    loss_total = 0.\n",
    "    num_correct_train = 0\n",
    "    for batch_ind, (input_ids, attention_mask, labels) in enumerate(dataloader_train):\n",
    "      if (batch_ind + 1) % 200 == 0: \n",
    "        print(batch_ind + 1, '/', len(dataloader_train), 'batches done')\n",
    "      input_ids = input_ids.to(device).long()\n",
    "      attention_mask = attention_mask.to(device)\n",
    "      labels = labels.to(device)      \n",
    "      logits, loss_batch_total = model(input_ids, attention_mask, labels) \n",
    "      preds = torch.where(logits > 0., 1, 0)  # 1 if p(1|x) > 0.5, 0 else\n",
    "      num_correct_train += (preds == labels).sum()\n",
    "      loss_total += loss_batch_total.item()            \n",
    "      \n",
    "      loss_batch_avg = loss_batch_total / input_ids.size(0)  \n",
    "      loss_batch_avg.backward()  \n",
    "\n",
    "      if clip > 0.:  # Optional gradient clipping\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "      optimizer.step()  # optimizer updates model weights based on stored gradients\n",
    "      scheduler.step()  # Update lr. \n",
    "      optimizer.zero_grad()  # Reset gradient slots to zero\n",
    "\n",
    "    # Useful training information\n",
    "    loss_avg = loss_total / len(dataloader_train.dataset)\n",
    "    acc_train = num_correct_train / len(dataloader_train.dataset) * 100.\n",
    "\n",
    "    # Check validation performance at the end of every epoch. \n",
    "    acc_val = get_acc_val(model, device)\n",
    "\n",
    "    if verbose:\n",
    "      print('Epoch {:3d} | avg loss {:8.4f} | train acc {:2.2f} | val acc {:2.2f}'.format(epoch + 1, loss_avg, acc_train, acc_val))\n",
    "\n",
    "    if acc_val > best_acc_val: \n",
    "      best_acc_val = acc_val\n",
    "  \n",
    "  if verbose: \n",
    "    print('Final avg loss {:8.4f} | final train acc {:2.2f} | best val acc {:2.2f}'.format(loss_avg, acc_train, best_acc_val))\n",
    "\n",
    "  return best_acc_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below Runs BERT on SST-2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 / 2105 batches done\n",
      "400 / 2105 batches done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-d3b3114f99c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mbest_acc_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-1925e002552f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, batch_size, num_warmup_steps, lr, num_epochs, clip, verbose, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mloss_batch_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch_total\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m       \u001b[0mloss_batch_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mclip\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Optional gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/topicbert/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/topicbert/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if True: # Set True to run. \n",
    "  set_seed(42)\n",
    "  model = BertClassifier()\n",
    "  best_acc_val = train(model, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
